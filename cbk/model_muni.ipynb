{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ead2d8",
   "metadata": {},
   "source": [
    "This notebook build predictive models for each municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694253bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, KFold, train_test_split\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "801ac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports function to add features to data.\n",
    "%run \"../helper_functions/add_features_to_data.py\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad42d16",
   "metadata": {},
   "source": [
    "A function for fitting a model is made. This function takes a dataset of cleaned Boliga data, and enrich it with the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc82b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_model(data):\n",
    "    # splitting data in target values (y) and features (X)\n",
    "    y = data[\"avg_sqm_price\"]\n",
    "    X = data.drop(columns=[\"avg_sqm_price\"])\n",
    "    \n",
    "    # numeric and categorical features are identified\n",
    "    numeric_features = X.select_dtypes(include = [\"number\"]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "    \n",
    "    # Data is split into test and training data, stratified on housing_type\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(.2), random_state=47, stratify=X[\"housing_type\"])\n",
    "    \n",
    "    # Known categories in the categorical data are identified and stored for use in OneHotEncoder\n",
    "    known_categories = [X[i].unique().tolist() for i in X.select_dtypes(include=[\"category\"]).columns.tolist()]\n",
    "    \n",
    "    # Preprocessor defined. Numerical features are scaled, and categorical values OneHotEncoded with the\n",
    "    # known categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(with_mean=False), numeric_features),\n",
    "        ('cat', OneHotEncoder(categories=known_categories), categorical_features)\n",
    "    ])\n",
    "\n",
    "    # The training pipeline is defined. Preprocessing as defined above, polynomial feature expansion\n",
    "    # and Elastic Net as the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('polynomial', PolynomialFeatures(degree=3)),  # Tilføj denne linje\n",
    "        ('classifier', ElasticNet())\n",
    "    ])\n",
    "\n",
    "    # Paramergrid defined for the gridsearch\n",
    "    param_grid = {\n",
    "        'polynomial__degree': [1, 2, 3],  # Ny linje for at prøve forskellige polynomial grader\n",
    "        'classifier__alpha': np.logspace(-4, 4, 12),\n",
    "        'classifier__l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "        'classifier__max_iter': [2000] \n",
    "    }\n",
    "    # Setting up the GridSearch with pipeline and parametergrid. 5-fold crossvalidation \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Searching for optimal hyperparameters.\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # grabbing information about the result\n",
    "    best_parameters = grid_search.best_params_\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = best_pipeline.score(X_test, y_test)\n",
    "    coefficients = best_pipeline.named_steps['classifier'].coef_\n",
    "\n",
    "    # Grabbing names and weights of the polynomial features.\n",
    "    # First, get names of both numeric and categorical features\n",
    "    numeric_feature_names = numeric_features\n",
    "    categorical_feature_names = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = np.concatenate([numeric_feature_names, categorical_feature_names])\n",
    "\n",
    "    # Now the polynomial feature names\n",
    "    polynomial_feature_names = best_pipeline.named_steps['polynomial'].get_feature_names_out(input_features=all_feature_names)\n",
    "\n",
    "    # Combining to one object\n",
    "    coefs =  zip(coefficients, polynomial_feature_names)\n",
    "    \n",
    "    # gets data for a learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator=best_pipeline,\n",
    "                   X=X_train,\n",
    "                   y=y_train,\n",
    "                   train_sizes=np.arange(0.05, 1.05, .05),\n",
    "                   scoring='neg_mean_squared_error',                 \n",
    "                   cv=10)\n",
    "    \n",
    "    learning_curve_data = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                     'Test':-test_scores.mean(axis=1),\n",
    "                     'sample size':train_sizes})\n",
    "    \n",
    "    # Finally return fitted models, parameters, metrics, coefficients and data for a learning curve\n",
    "    return (grid_search, best_parameters, rmse, r2, coefs, learning_curve_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5b70396",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████▎                                     | 53/98 [51:11<43:27, 57.95s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m add_features_to_data(data) \u001b[38;5;66;03m# feature adding\u001b[39;00m\n\u001b[0;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuni_code\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# dropping columns\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m grid_search, best_parameters, rmse, r2, coefs, learning_curve_data \u001b[38;5;241m=\u001b[39m \u001b[43mmake_a_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# saving pickled models\u001b[39;00m\n\u001b[0;32m     21\u001b[0m rick \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(grid_search)\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mmake_a_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Data is split into test and training data, stratified on housing_type\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m47\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhousing_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Known categories in the categorical data are identified and stored for use in OneHotEncoder\u001b[39;00m\n\u001b[0;32m     14\u001b[0m known_categories \u001b[38;5;241m=\u001b[39m [X[i]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()]\n",
      "File \u001b[1;32mc:\\program files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2420\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\program files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2095\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2098\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2099\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2102\u001b[0m     )\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Getting aggregated data stored as csv's\n",
    "fp = Path(\"../Boliga data/agg_data/\")\n",
    "files = list(fp.glob('*.csv'))\n",
    "\n",
    "# initialising dataframes for saving results of the fits\n",
    "metrics = pd.DataFrame(columns=['muni_code', 'rmse', 'r2'])\n",
    "fitted_models = pd.DataFrame(columns=['muni_code', 'pickled_model'])\n",
    "learning_curves = pd.DataFrame(columns=['muni_code',  \"Train\",\"Test\", \"sample size\"])\n",
    "coefficients = pd.DataFrame(columns=['muni_code', 'value', 'parameter'])\n",
    "parameters = pd.DataFrame(columns=['muni_code', 'Parameter', 'Value']) \n",
    "\n",
    "# running the loop for modelling\n",
    "for filename in tqdm.tqdm(files):\n",
    "    muni_code = re.search(r'(\\d+)\\.csv$', str(filename)).group(1) # extracting muni_code\n",
    "    data = pd.read_csv(filename) # reading data\n",
    "    data = add_features_to_data(data) # feature adding\n",
    "    data = data.drop(columns=[\"year\", \"muni_code\"]) # dropping columns\n",
    "    grid_search, best_parameters, rmse, r2, coefs, learning_curve_data = make_a_model(data)\n",
    "    \n",
    "    # saving pickled models\n",
    "    rick = pickle.dumps(grid_search)\n",
    "    model_row = pd.DataFrame({'muni_code': [muni_code],\n",
    "             'pickled_model':[rick]})\n",
    "    fitted_models = fitted_models.append(model_row, ignore_index = True)\n",
    "    \n",
    "    # saving metrics\n",
    "    metric_tuple = (muni_code, rmse, r2)\n",
    "     # Konverter tuple til DataFrame\n",
    "    metric_row = pd.DataFrame([metric_tuple], columns=metrics.columns)\n",
    "    # Append til den eksisterende DataFrame\n",
    "    metrics = metrics.append(metric_row, ignore_index = True)\n",
    "    \n",
    "    # saving parameters\n",
    "    param_row = pd.DataFrame(list(best_parameters.items()), columns=['Parameter', 'Value'])\n",
    "    param_row['muni_code'] = muni_code\n",
    "    # append\n",
    "    parameters = parameters.append(param_row, ignore_index = True)\n",
    "    # saving coefficients\n",
    "    coef_row = pd.DataFrame(coefs, columns = ['value', 'parameter'])\n",
    "    coef_row['muni_code'] = muni_code\n",
    "    coefficients = coefficients.append(coef_row, ignore_index = True)\n",
    "    # saving learning curve data\n",
    "    learning_curve_data['muni_code'] = muni_code\n",
    "    learning_curves = learning_curves.append(learning_curve_data, ignore_index = True)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e43805",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# fitted_models \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mparameters\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter_muni.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m coefficients\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients_muni.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m metrics\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics_muni.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# fitted_models \n",
    "\n",
    "parameters.to_csv(\"parameter_muni.csv\")\n",
    "coefficients.to_csv(\"coefficients_muni.csv\")\n",
    "metrics.to_csv(\"metrics_muni.csv\")\n",
    "learning_curves.to_csv(\"learning_curves_muni.csv\")\n",
    "fitted_models.to_csv(\"fitted_models_muni.csv\")\n",
    "parameters.to_csv(\"parameters_muni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2b0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e06b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93 entries, 0 to 92\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   muni_code      93 non-null     int64  \n",
      " 1   housing_type   93 non-null     object \n",
      " 2   year           93 non-null     int64  \n",
      " 3   avg_sqm_price  93 non-null     float64\n",
      " 4   count          93 non-null     int64  \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 3.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>muni_code</th>\n",
       "      <th>housing_type</th>\n",
       "      <th>year</th>\n",
       "      <th>avg_sqm_price</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153</td>\n",
       "      <td>Ejerlejlighed</td>\n",
       "      <td>1</td>\n",
       "      <td>13602.037037</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>Ejerlejlighed</td>\n",
       "      <td>2</td>\n",
       "      <td>13089.625000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153</td>\n",
       "      <td>Ejerlejlighed</td>\n",
       "      <td>3</td>\n",
       "      <td>14703.173913</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153</td>\n",
       "      <td>Ejerlejlighed</td>\n",
       "      <td>4</td>\n",
       "      <td>13545.869565</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>Ejerlejlighed</td>\n",
       "      <td>5</td>\n",
       "      <td>12148.500000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>153</td>\n",
       "      <td>Villa</td>\n",
       "      <td>27</td>\n",
       "      <td>13967.101266</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>153</td>\n",
       "      <td>Villa</td>\n",
       "      <td>28</td>\n",
       "      <td>15152.757396</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>153</td>\n",
       "      <td>Villa</td>\n",
       "      <td>29</td>\n",
       "      <td>14220.593023</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>153</td>\n",
       "      <td>Villa</td>\n",
       "      <td>30</td>\n",
       "      <td>14596.116564</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>153</td>\n",
       "      <td>Villa</td>\n",
       "      <td>31</td>\n",
       "      <td>13462.322917</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    muni_code   housing_type  year  avg_sqm_price  count\n",
       "0         153  Ejerlejlighed     1   13602.037037     27\n",
       "1         153  Ejerlejlighed     2   13089.625000     24\n",
       "2         153  Ejerlejlighed     3   14703.173913     23\n",
       "3         153  Ejerlejlighed     4   13545.869565     23\n",
       "4         153  Ejerlejlighed     5   12148.500000     18\n",
       "..        ...            ...   ...            ...    ...\n",
       "88        153          Villa    27   13967.101266    158\n",
       "89        153          Villa    28   15152.757396    169\n",
       "90        153          Villa    29   14220.593023    172\n",
       "91        153          Villa    30   14596.116564    163\n",
       "92        153          Villa    31   13462.322917     96\n",
       "\n",
       "[93 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_plot(tester):\n",
    "    f_learn, ax = plt.subplots(figsize=(7,3))\n",
    "    ax.plot(tester[\"sample size\"],np.sqrt(tester[\"Test\"]), alpha=0.25, linewidth=2, label ='Test', color='blue') # negated, because we already use neg_MSE\n",
    "    ax.plot(tester[\"sample size\"],np.sqrt(tester[\"Train\"]), alpha=0.25, linewidth=2, label='Train', color='orange') # negated, because we already use neg_MSE\n",
    "\n",
    "    ax.set_title('Mean performance')\n",
    "    ax.set_ylabel('Root-Mean squared error')\n",
    "    ax.legend();\n",
    "learning_curve_plot(tester)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
