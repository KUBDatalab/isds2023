{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5839f-7b92-4439-922e-a0cbf434bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cc047-f52c-40dc-b02b-819204f139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct variables to be used later\n",
    "# municipality_code and pages_total must be updated for each scrape\n",
    "\n",
    "municipality_code = 101 #number code of municipality\n",
    "pages_total = 5953 #amount of pages in search results\n",
    "\n",
    "year_min = 1992 #start year for scrape\n",
    "year_max = 2022 #end year for scrape\n",
    "\n",
    "url_0 = 'https://www.boliga.dk/salg/resultater?searchTab=1&propertyType=1,2,3&salesDateMin=' # first part of link to the website\n",
    "url_1 = '&salesDateMax='\n",
    "url_2 = '&municipality='\n",
    "url_end = '&sort=date-d&page=' # second part of link to the website\n",
    "\n",
    "logfile = f'log_{municipality_code}.csv' #name of logfile based on municipality_code\n",
    "list_htmls = [] #list for storing scraped urls\n",
    "\n",
    "file_title = f'sales_{year_min}_{year_max}_{municipality_code}' #create string to be used in file names\n",
    "globals()[file_title] = pd.DataFrame() #create df based on file_title and municipality_code\n",
    "\n",
    "#%who #see which variables are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e9578-0c56-45b3-bffc-891635a94239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the log function to gather the log information\n",
    "def log(response,logfile,output_path=os.getcwd()):\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(logfile): #If the log file exists, open it and allow for changes     \n",
    "        log = open(logfile,'a')\n",
    "    else: #If the log file does not exist, create it and make headers for the log variables\n",
    "        log = open(logfile,'w')\n",
    "        header = ['timestamp','status_code','length','output_file']\n",
    "        log.write(';'.join(header) + \"\\n\") #Make the headers and jump to new line\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code #Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #Local time\n",
    "    length = len(response.text) #Length of the HTML-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open(logfile,'a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{url}' + \"\\n\") #Append the information and jump to new line"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc72d3dd-4031-4afc-8058-a3f1502f36ef",
   "metadata": {},
   "source": [
    "def get_last_page_no(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    nav_right_element = soup.find(class_='nav-right')\n",
    "    \n",
    "    if nav_right_element:\n",
    "        page_button_element = nav_right_element.find(class_='page-button')\n",
    "        last_page = int(page_button_element.text)\n",
    "        return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc87dc-61f4-4725-b09a-7ece6ea74b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop runs through all pages based on municipality_code, pages_total, year_min and year_max\n",
    "# Then stores the results in a dataframe and saves the result in a csv-file, both named according to years and municipality code\n",
    "for i in tqdm.tqdm(range(5350, pages_total)):\n",
    "    page_no = i + 1\n",
    "    url = f'{url_0}{year_min}{url_1}{year_max}{url_2}{municipality_code}{url_end}{page_no}'\n",
    "    random_no =  random.randrange(5, 10, 1) / 10\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'name':'Daniel Pryn'\\\n",
    "                                              ,'email':'knl810@alumni.ku.dk'\\\n",
    "                                              ,'message':'This is solely used for a university project. If we cause you any inconvenience, please let us know.'})\n",
    "    except Exception as e:\n",
    "        print(url) #Print url\n",
    "        print(e) #Print error\n",
    "        #sales_1992_2022_165.to_csv('sales_1992_2022_165.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "        continue #Continue to next iteration of the loop\n",
    "    \n",
    "    if response.ok: #Check if the response carries any data\n",
    "        tables = pd.read_html(url, encoding='utf-8') #If the response carries data, then save the tables\n",
    "    else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "        print(url)\n",
    "        print(response.status_code)\n",
    "        continue\n",
    "    \n",
    "    result_df = pd.DataFrame(tables[0]) #Convert this iteration's first table to a dataframe\n",
    "    globals()[file_title] = pd.concat([globals()[file_title],result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "    log(response, logfile)\n",
    "    \n",
    "    if (page_no % 10 == 0) or (page_no == pages_total): #save results to csv for every 10 pages and after last page\n",
    "        globals()[file_title].to_csv(f'{file_title}.csv', index=False)\n",
    "        \n",
    "    #time.sleep(random_no) #Sleep between 0.5 and 1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e08c7b-2eb3-401f-be7e-84939cfb70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV file\n",
    "results_log = pd.read_csv(f'log_{municipality_code}.csv')\n",
    "results = pd.read_csv(f'{file_title}.csv')\n",
    "  \n",
    "# count no. of lines\n",
    "print(\"Number of lines present:-\", \n",
    "      len(results))\n",
    "print(\"Number of urls present:-\", \n",
    "      len(results_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c1f9b-e6db-46ea-b4a0-7cfc61a85cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dc4b0-93f3-4705-9cce-571fbce48f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c57513-5997-49df-93f8-5de63dd7bb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7801c-22c7-41b2-b3ee-13124af6e41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027fb5c-d610-4510-9c92-c88285632671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals()[file_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539855e9-4440-438a-b543-973bcbcf4298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b2299-bc60-45e3-9c94-b3a9b7cd870b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "526611c6-3e4b-4b89-9f66-9516142dae28",
   "metadata": {},
   "source": [
    "logfile = f'log_{municipality_code}.csv' #name of logfile based on municipality_code\n",
    "list_htmls = [] #list for storing scraped urls\n",
    "file_title = f'{base_title}{municipality_code}'\n",
    "globals()[base_title + str(municipality_code)] = pd.DataFrame() #create df based on base_title and municipality_code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac4432c0-f613-4234-9125-d5b3fafe5043",
   "metadata": {},
   "source": [
    "for i in tqdm.tqdm(range(153)):\n",
    "    page_no = i + 1\n",
    "    url = f'{base_url}{page_no}'\n",
    "    random_no =  random.randrange(5, 20, 1) / 10\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'name':'Daniel Pryn'\\\n",
    "                                              ,'email':'knl810@alumni.ku.dk'\\\n",
    "                                              ,'message':'This is solely used for a university project. If we cause you any inconvenience, please let us know.'})\n",
    "    except Exception as e:\n",
    "        print(url) #Print url\n",
    "        print(e) #Print error\n",
    "        #sales_1992_2022_165.to_csv('sales_1992_2022_165.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "        continue #Continue to next iteration of the loop\n",
    "    \n",
    "    if response.ok: #Check if the response carries any data\n",
    "        tables = pd.read_html(url, encoding='utf-8') #If the response carries data, then save the tables\n",
    "    else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "        print(response.status_code)\n",
    "        continue\n",
    "    \n",
    "    result_df = pd.DataFrame(tables[0]) #Convert this iteration's first table to a dataframe\n",
    "    sales_1992_2022_165 = pd.concat([sales_1992_2022_165,result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "    log(response, logfile)\n",
    "    \n",
    "    if (page_no % 10 == 0) or (page_no == 153):\n",
    "        sales_1992_2022_165.to_csv('sales_1992_2022_165.csv', index=False)\n",
    "        \n",
    "    time.sleep(random_no) #Sleep between 0.5 and 2 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
