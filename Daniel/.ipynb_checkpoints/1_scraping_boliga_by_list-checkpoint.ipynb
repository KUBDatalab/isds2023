{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5839f-7b92-4439-922e-a0cbf434bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cc047-f52c-40dc-b02b-819204f139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct variables to be used later\n",
    "\n",
    "year_min = 1992 #start year for scrape\n",
    "year_max = 2022 #end year for scrape\n",
    "\n",
    "url_0 = 'https://www.boliga.dk/salg/resultater?searchTab=1&propertyType=1,2,3&salesDateMin=' # first part of link to the website\n",
    "url_1 = '&salesDateMax='\n",
    "url_2 = '&municipality='\n",
    "url_end = '&sort=date-d&page=' # second part of link to the website\n",
    "\n",
    "# Create Path object of new folder located inside the current working directory of this notebook\n",
    "fp = Path.cwd() / 'data'  \n",
    "# Use the Path object to actually create the subfolder\n",
    "Path.mkdir(fp, exist_ok=True) \n",
    "\n",
    "#%who #see which variables are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a5634-0674-4b10-9f14-3e087cbcd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality_list = [\n",
    "    165, 201, 151, 400, 153, 155, 240, 210, 147, 250, 190, 157, 159, 161,\n",
    "    270, 260, 217, 163, 219, 167, 169, 223, 183, 101, 173, 230, 175, 185,\n",
    "    187, 320, 253, 376, 316, 326, 259, 350, 360, 370, 306, 329, 265, 330,\n",
    "    269, 340, 336, 390, 420, 530, 561, 563, 607, 430, 510, 440, 621, 482,\n",
    "    410, 480, 450, 461, 479, 540, 550, 573, 575, 630, 492, 580, 710, 766,\n",
    "    657, 661, 615, 756, 665, 707, 727, 730, 760, 741, 740, 746, 779, 671,\n",
    "    706, 791, 751, 810, 813, 860, 849, 825, 846, 773, 840, 787, 820, 851\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e9578-0c56-45b3-bffc-891635a94239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the log function to gather the log information\n",
    "def log(response,logfile):\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(f'{fp}/{logfile}'): #If the log file exists, open it and allow for changes     \n",
    "        log = open(f'{fp}/{logfile}','a')\n",
    "    else: #If the log file does not exist, create it and make headers for the log variables\n",
    "        log = open(f'{fp}/{logfile}','w')\n",
    "        header = ['timestamp','status_code','length','output_file']\n",
    "        log.write(';'.join(header) + \"\\n\") #Make the headers and jump to new line\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code #Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #Local time\n",
    "    length = len(response.text) #Length of the HTML-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open(f'{fp}/{logfile}','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{url}' + \"\\n\") #Append the information and jump to new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cb62b-c5b4-45b5-92c0-48d75f674b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the last page number of a search based on a given municipality code\n",
    "def get_last_page_no(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    nav_right_element = soup.find(class_='nav-right')\n",
    "    \n",
    "    if nav_right_element:\n",
    "        page_button_element = nav_right_element.find(class_='page-button')\n",
    "        last_page = int(page_button_element.text)\n",
    "        return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24fb27-43f6-45d1-bffd-39c98d13c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in municipality_list:\n",
    "    \n",
    "    municipality_code = code\n",
    "    logfile = f'log_{municipality_code}.csv' #name of logfile based on municipality_code\n",
    "    list_htmls = [] #list for storing scraped urls\n",
    "\n",
    "    file_title = f'sales_{year_min}_{year_max}_{municipality_code}' #create string to be used in file names\n",
    "    globals()[file_title] = pd.DataFrame() #create df named from file_title\n",
    "    url = f'{url_0}{year_min}{url_1}{year_max}{url_2}{municipality_code}{url_end}1' #set start url based on municipality_code\n",
    "    pages_total = get_last_page_no(url) #find last page number using get_last_page() function\n",
    "    \n",
    "    # This loop runs through all pages based on municipality_code, pages_total, year_min and year_max\n",
    "    # Then stores the results in a dataframe and saves the result in a csv-file, both named according to years and municipality code\n",
    "    for i in tqdm.tqdm(range(pages_total)):\n",
    "        \n",
    "        page_no = i + 1\n",
    "        url = f'{url_0}{year_min}{url_1}{year_max}{url_2}{municipality_code}{url_end}{page_no}' #overwrites url for each page iteration\n",
    "        #random_no =  random.randrange(2, 7, 1) / 10\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers={'name':'Daniel Pryn'\\\n",
    "                                                  ,'email':'knl810@alumni.ku.dk'\\\n",
    "                                                  ,'message':'This is solely used for a university project. If we cause you any inconvenience, please let us know.'})\n",
    "        except Exception as e:\n",
    "            print(url) #Print url\n",
    "            print(e) #Print error\n",
    "            #sales_1992_2022_165.to_csv('sales_1992_2022_165.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "            continue #Continue to next iteration of the loop\n",
    "        \n",
    "        if response.ok: #Check if the response carries any data\n",
    "            tables = pd.read_html(url, encoding='utf-8') #If the response carries data, then save the tables\n",
    "        else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "            print(url)\n",
    "            print(response.status_code)\n",
    "            continue\n",
    "        \n",
    "        result_df = pd.DataFrame(tables[0]) #Convert this iteration's first table to a dataframe\n",
    "        globals()[file_title] = pd.concat([globals()[file_title],result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "        log(response, logfile) #call log() and write to log_{municipality_code}.csv\n",
    "        \n",
    "        if (page_no % 10 == 0) or (page_no == pages_total): #save results to csv for every 10 pages and after last page\n",
    "            globals()[file_title].to_csv(f'{fp}/{file_title}.csv', index=False)\n",
    "            \n",
    "        #time.sleep(random_no) #Sleep between 0.2 and 0.7 seconds\n",
    "\n",
    "    #The code below gives a print after each full municipality harvest, so that progress can be monitored and validated\n",
    "    results_listings = pd.read_csv(f'{fp}/{file_title}.csv')\n",
    "    results_log = pd.read_csv(f'{fp}/log_{municipality_code}.csv')\n",
    "      \n",
    "    # count no. of pages and listings\n",
    "    print(f'Number of pages harvested for {municipality_code}:-', \n",
    "          len(results_log))\n",
    "    print(f'Number of listings harvested for {municipality_code}: ', \n",
    "          len(results_listings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31608131-74f1-4ba0-b972-b19a30f9b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals()[file_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0569f-011c-4f95-8590-8b5cfce55918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "583369da-0574-48be-bed1-05a30cfddc1a",
   "metadata": {},
   "source": [
    "#old code\n",
    "\n",
    "for code in municipality_list[37:40]:\n",
    "    \n",
    "    municipality_code = code\n",
    "    logfile = f'log_{municipality_code}.csv' #name of logfile based on municipality_code\n",
    "    list_htmls = [] #list for storing scraped urls\n",
    "\n",
    "    file_title = f'sales_{year_min}_{year_max}_{municipality_code}' #create string to be used in file names\n",
    "    globals()[file_title] = pd.DataFrame() #create df based on file_title and municipality_code\n",
    "    url = f'{url_0}{year_min}{url_1}{year_max}{url_2}{municipality_code}{url_end}1' #get url to find last page number\n",
    "    pages_total = get_last_page_no(url) #find last page number using get_last_page() function\n",
    "    \n",
    "    # This loop runs through all pages based on municipality_code, pages_total, year_min and year_max\n",
    "    # Then stores the results in a dataframe and saves the result in a csv-file, both named according to years and municipality code\n",
    "    for i in tqdm.tqdm(range(pages_total)):\n",
    "\n",
    "        threshold_time = 120  # Adjust this value as needed\n",
    "        start_time = time.time()  # Get the start time of the iteration\n",
    "        \n",
    "        page_no = i + 1\n",
    "        url = f'{url_0}{year_min}{url_1}{year_max}{url_2}{municipality_code}{url_end}{page_no}' #overwrites url for each page iteration\n",
    "        random_no =  random.randrange(2, 7, 1) / 10\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers={'name':'Daniel Pryn'\\\n",
    "                                                  ,'email':'knl810@alumni.ku.dk'\\\n",
    "                                                  ,'message':'This is solely used for a university project. If we cause you any inconvenience, please let us know.'})\n",
    "        except Exception as e:\n",
    "            print(url) #Print url\n",
    "            print(e) #Print error\n",
    "            #sales_1992_2022_165.to_csv('sales_1992_2022_165.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "            continue #Continue to next iteration of the loop\n",
    "        \n",
    "        if response.ok: #Check if the response carries any data\n",
    "            tables = pd.read_html(url, encoding='utf-8') #If the response carries data, then save the tables\n",
    "        else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "            print(url)\n",
    "            print(response.status_code)\n",
    "            continue\n",
    "        \n",
    "        result_df = pd.DataFrame(tables[0]) #Convert this iteration's first table to a dataframe\n",
    "        globals()[file_title] = pd.concat([globals()[file_title],result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "        log(response, logfile)\n",
    "        \n",
    "        if (page_no % 10 == 0) or (page_no == pages_total): #save results to csv for every 10 pages and after last page\n",
    "            globals()[file_title].to_csv(f'{fp}/{file_title}.csv', index=False)\n",
    "            \n",
    "        #time.sleep(random_no) #Sleep between 0.5 and 1 seconds\n",
    "\n",
    "        # Calculate the elapsed time for the iteration\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Check if the elapsed time exceeds the threshold\n",
    "        if elapsed_time > threshold_time:\n",
    "            print('Elapsed time exceeded threshold for:')\n",
    "            print(url)\n",
    "            print('Skipping to next iteration.')\n",
    "            continue  # Move to the next iteration\n",
    "\n",
    "    results_listings = pd.read_csv(f'{fp}/{file_title}.csv')\n",
    "    results_log = pd.read_csv(f'{fp}/log_{municipality_code}.csv')\n",
    "      \n",
    "    # count no. of pages and listings\n",
    "    print(f'Number of pages harvested for {municipality_code}:-', \n",
    "          len(results_log))\n",
    "    print(f'Number of listings harvested for {municipality_code}: ', \n",
    "          len(results_listings))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32e78ca0-b122-4bd1-899e-b9ca3fac0f22",
   "metadata": {},
   "source": [
    "# read CSV file\n",
    "results_log = pd.read_csv(f'log_{municipality_code}.csv')\n",
    "results = pd.read_csv(f'{file_title}.csv')\n",
    "  \n",
    "# count no. of lines\n",
    "print(\"Number of lines present:-\", \n",
    "      len(results))\n",
    "print(\"Number of urls present:-\", \n",
    "      len(results_log))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8add0d23-ab4f-40b7-99d7-774a4f8836e4",
   "metadata": {},
   "source": [
    "# read CSV file\n",
    "results_log = pd.read_csv(f'log_157.csv')\n",
    "results = pd.read_csv(f'sales_1992_2022_157.csv')\n",
    "  \n",
    "# count no. of lines\n",
    "print(\"Number of lines present:-\", \n",
    "      len(results))\n",
    "print(\"Number of urls present:-\", \n",
    "      len(results_log))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d3d2c24-2a45-4b4c-98b0-8ef1bc4e0831",
   "metadata": {},
   "source": [
    "globals()[file_title].to_csv(f'{file_title}.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "526611c6-3e4b-4b89-9f66-9516142dae28",
   "metadata": {},
   "source": [
    "logfile = f'log_{municipality_code}.csv' #name of logfile based on municipality_code\n",
    "list_htmls = [] #list for storing scraped urls\n",
    "file_title = f'{base_title}{municipality_code}'\n",
    "globals()[base_title + str(municipality_code)] = pd.DataFrame() #create df based on base_title and municipality_code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac4432c0-f613-4234-9125-d5b3fafe5043",
   "metadata": {},
   "source": [
    "for i in tqdm.tqdm(range(153)):\n",
    "    page_no = i + 1\n",
    "    url = f'{base_url}{page_no}'\n",
    "    random_no =  random.randrange(5, 20, 1) / 10\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'name':'Daniel Pryn'\\\n",
    "                                              ,'email':'knl810@alumni.ku.dk'\\\n",
    "                                              ,'message':'This is solely used for a university project. If we cause you any inconvenience, please let us know.'})\n",
    "    except Exception as e:\n",
    "        print(url) #Print url\n",
    "        print(e) #Print error\n",
    "        #sales_1992_2022_165.to_csv('sales_1992_2022_165.csv') #Save the dataframe as a csv file to retrieve at another time\n",
    "        continue #Continue to next iteration of the loop\n",
    "    \n",
    "    if response.ok: #Check if the response carries any data\n",
    "        tables = pd.read_html(url, encoding='utf-8') #If the response carries data, then save the tables\n",
    "    else: #If the response does not carry any data, then print the status_code and continue to next iteration of the loop\n",
    "        print(response.status_code)\n",
    "        continue\n",
    "    \n",
    "    result_df = pd.DataFrame(tables[0]) #Convert this iteration's first table to a dataframe\n",
    "    sales_1992_2022_165 = pd.concat([sales_1992_2022_165,result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "    log(response, logfile)\n",
    "    \n",
    "    if (page_no % 10 == 0) or (page_no == 153):\n",
    "        sales_1992_2022_165.to_csv('sales_1992_2022_165.csv', index=False)\n",
    "        \n",
    "    time.sleep(random_no) #Sleep between 0.5 and 2 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
